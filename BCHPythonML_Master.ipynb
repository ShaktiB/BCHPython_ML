{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python: Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: # Display Matplotlib graphs within the Notebook (and note as separate window pop-ups)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder  \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pd.set_option('display.max_columns', None) # Display all columns when there are a lot of columns in dataframe\n",
    "%matplotlib inline # Display Matplotlib graphs within the Notebook (and note as separate window pop-ups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/loan_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "- EDA is an important step in the ML/Data Science pipeline \n",
    "- Gain a high-level understanding of the data and its characteristics (data types, rows, columns, missing values, etc.)  \n",
    "- This step helps provide guidance on how to pre-process the data to prep it for model building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data about the data (nulls, data types, rows/columns, etc.)\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing Values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical summary for the data \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Unique Values in all of the categorical columns \n",
    "categorical_cols = ['Gender','Married','Education','Self_Employed', 'Property_Area']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f'Unique Values for {col}: {df[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any repeated records with regards to Loan ID\n",
    "len(df.Loan_ID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Summarize! \n",
    "- Loan ID is the primary key in the data - it uniquely identifies each record \n",
    "- There are 614 rows, 13 columns\n",
    "- The .describe() function can be used to quickly gauge some statistics about the data \n",
    "    - In some cases it can also help identify some incorrect data (if this was a biometric dataset with heart-rate, an minimum heartrate of 0 would be a call for investigation!) \n",
    "- 7/13 columns have missing values \n",
    "- Credit History has the highest number of missing values! \n",
    "\n",
    "#### Key Remarks \n",
    "- Understanding the data you are working with is very important! \n",
    "- Always strive to work with Subjet Matter Experts (SMEs) to get insight into the data \n",
    "- In a real-world application, you may need to individually evaluate each column and its values to learn the context behind the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis / Data Visualization\n",
    "- Investigate to find relationships and trends within the data \n",
    "- Certain features may be more prominent in determining whether the applicant's loan with be approved or not \n",
    "- Data Visualization can help reveal key information in the data \n",
    "    - Knowing which graphs to use is a key skills that comes with practice and experience! \n",
    "- A good starting point is compare different features against the label (Loan Status) to see if there are any easily distinguishable relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Approved & Not Approved (Y/N) records \n",
    "df.Loan_Status.value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Loan_Status.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender vs Approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand how different\n",
    "print(pd.crosstab(df['Gender'],df['Loan_Status']))\n",
    "\n",
    "sns.countplot(df['Gender'],hue=df['Loan_Status'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(df['Gender'], df['Loan_Status']).apply(lambda r: round(r/r.sum(),3)*100, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct = df.groupby('Gender')['Loan_Status'].value_counts(normalize=True)\n",
    "df_pct = df_pct.mul(100)\n",
    "df_pct = df_pct.rename('percent').reset_index()\n",
    "\n",
    "sns.catplot(x='Gender',y='percent',hue='Loan_Status',kind='bar',data=df_pct)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write this as a method to make it easy to check the loan status against all the parameters\n",
    "def column_bar(df, column):\n",
    "    print(pd.crosstab(df[column],df['Loan_Status']))\n",
    "    print('\\nPercentage')\n",
    "    print(pd.crosstab(df[column], df['Loan_Status']).apply(lambda r: round(r/r.sum(),3)*100, axis=1))\n",
    "    sns.countplot(df[column],hue=df['Loan_Status'])\n",
    "    plt.show()\n",
    "    df_pct = df.groupby(column)['Loan_Status'].value_counts(normalize=True)\n",
    "    df_pct = df_pct.mul(100)\n",
    "    df_pct = df_pct.rename('percent').reset_index()\n",
    "\n",
    "    sns.catplot(x=column,y='percent',hue='Loan_Status',kind='bar',data=df_pct)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function \n",
    "column_bar(df, 'Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Categorical Features vs Approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the data we want to test\n",
    "columns = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Credit_History']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns:\n",
    "    column_bar(df,column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Continuous variables\n",
    "def column_scatter(df,column):\n",
    "    plt.scatter(df[column], df['Loan_Status']);\n",
    "    plt.title(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ApplicantIncome\n",
    "# CoapplicantIncome\n",
    "# LoanAmount\n",
    "# Loan_Amount_Term\n",
    "\n",
    "scatter_columns= ['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term']\n",
    "for column in scatter_columns:\n",
    "    column_scatter(df,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram -----> Review, should we keep this? \n",
    "\n",
    "y_loan = df.loc[df.Loan_Status == 'Y']\n",
    "n_loan = df.loc[df.Loan_Status == 'N']\n",
    "\n",
    "kwargs = dict(alpha=0.5, bins=50)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(y_loan['ApplicantIncome'], **kwargs, color='g', label='Yes')\n",
    "plt.hist(n_loan['ApplicantIncome'], **kwargs, color='r', label='No')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at correlation next\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null Values\n",
    "- There are many ways to deal with NULL values and it can have a significant impact on how your model performs\n",
    "    - Deleting rows\n",
    "    - Replacing with Mean, Median, Mode\n",
    "    - Imputing values (KNN, ML algorithms, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets re-check columns with null values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets investigate the Loan Amount \n",
    "df['LoanAmount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets investigate the Loan Amount field \n",
    "plt.hist(df['LoanAmount'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Loan Amount NULL values with Mean\n",
    "df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove remaining records with Null values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm it worked\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicate entries\n",
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What's the shape of the new data?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Values\n",
    "- ML models can only deal with numerical values \n",
    "- Categorical data has to be encoded as numbers for use in models \n",
    "- Common techniques: Ordinal Encoding & One-Hot Encoding\n",
    "    - We will us the **get_dummies()** function in Pandas to do this, however when building ML for projects, using the **LabelEncoder & OneHotEncoder** modules in Sklearn are recommended \n",
    "    - Using get_dummies() functionally creates the same result, and is quicker to easily visualize the concept\n",
    "- When dealing with categorical data in production, additional solutions/algorithms may be required to deal with unseen categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to replace string data (Y,N), with numbers\n",
    "df['Loan_Status'].replace('N',0,inplace=True)\n",
    "df['Loan_Status'].replace('Y',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encod the features using get_dummies() function in Pandas \n",
    "\n",
    "non_numerical = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area']\n",
    "\n",
    "for column in non_numerical:\n",
    "    enc_df = pd.get_dummies(df[column])\n",
    "    print(f'{df[column].unique()}')\n",
    "    df = pd.concat([df,enc_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "le = LabelEncoder()\n",
    "non_numerical = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area']\n",
    "\n",
    "for column in non_numerical:\n",
    "    print(le.fit(df[column]).classes_)\n",
    "    df[column+'_Encoded'] = le.fit_transform(df[column])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create an instance of a one-hot-encoder \n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#for column in non_numerical:\n",
    "enc_df = pd.DataFrame(enc.fit_transform(df[['Gender_Encoded']]).toarray())\n",
    "    \n",
    "    # Merge with main df on key_values \n",
    "df = pd.merge(df, enc_df, left_index=True, right_index=True)\n",
    "\n",
    "df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Heatmap Again\n",
    "plt.figure(figsize=(16,5))\n",
    "sns.heatmap(df.corr(),annot=True,cmap='jet')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "- After analyzing the data, select the features you will use to help build the model \n",
    "- You do not always need to use every single feature. With lots of data, removing unnecessary features can save processing time, save costs, and even improve model performance\n",
    "- Since the categorical features have been encoded, drop the respective non-encoded categorical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obvious parameter to drop\n",
    "df.drop(['Loan_ID','Gender','Married','Dependents','Education','Self_Employed','Property_Area','Female','Male'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into target and features\n",
    "Y = df['Loan_Status'].to_frame()\n",
    "X = df.drop(columns=['Loan_Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.25\n",
    "seed = 12\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y,test_size=validation_size,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print('\\nTrain & Test Class Counts\\n')\n",
    "\n",
    "print('Training:\\n',Y_train.Loan_Status.value_counts())\n",
    "print('\\nTesting:\\n',Y_test.Loan_Status.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instances \n",
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "models = [lr, knn] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training & Testing & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation tools\n",
    "from sklearn.metrics import classification_report, roc_auc_score, plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    # Train\n",
    "    model.fit(X_train, Y_train)\n",
    "    print(f'\\nDone Training: {model}!')\n",
    "    \n",
    "    # Test\n",
    "    print(f'Mean Accuracy: {model.score(X_test,Y_test)}\\n')\n",
    "    \n",
    "    # Evaluate \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_pred,Y_test))\n",
    "    print(f'ROC Score:{roc_auc_score(Y_test,y_pred)}')\n",
    "    \n",
    "    plot_roc_curve(model, X_test, Y_test)\n",
    "    plt.show()\n",
    "#lr_results = models[1].predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks\n",
    "\n",
    "- The average score is not always a true representation of how good a model is, especially for classification\n",
    "- What if the model has to evaluate between apples & oranges, given there are 90 apples & 10 oranges ? \n",
    "    - If the model correctly classifies 90 apples, but only 5/10 organges are correctly classified, the model would still have a high accuracy even though it clearly cannot be trusted to properly classrify oranges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations \n",
    " - Test out different algorithms -> Support Vector Machine\n",
    " - Iterate over the feature selection process\n",
    " - Feature Engineering: Develop your own features from the available data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
